{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import KDTree\n",
    "import numpy as np\n",
    "\n",
    "data_path = 'Data/Input/points.txt'\n",
    "K = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filepath):\n",
    "    '''Function to read a file and output points\n",
    "    \n",
    "    Arguments:\n",
    "    1. filepath: the path to the file to read\n",
    "    \n",
    "    Returns:\n",
    "    1. points: a numpy array containing points'''\n",
    "    # data input done\n",
    "    points = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "    for line in lines:\n",
    "        pt = list(map(float, line.strip().split(',')))\n",
    "        points.append(pt)\n",
    "        \n",
    "    points = np.array(points)\n",
    "    return points\n",
    "\n",
    "\n",
    "def init_centroids(data, k):\n",
    "    '''Function to initialize 'k' random centroids from the data\n",
    "    \n",
    "    Arguments:\n",
    "    1. data: input data for kmeans.\n",
    "    2. k: number of centroids to output\n",
    "    \n",
    "    Returns:\n",
    "    1. a (k x dim) array containing randomly initialized centroids'''\n",
    "    ind = np.random.choice(np.arange(len(data)), size = k)\n",
    "    return data[ind]\n",
    "\n",
    "def map_(fpath, indices, centroid):\n",
    "    pts = read_file(fpath)[indices]  \n",
    "    tree = KDTree(centroid)\n",
    "    _, cent_ind = tree.query(pts)\n",
    "    value = pts       # (x, 1) pairs\n",
    "    return cent_ind, value\n",
    "\n",
    "def partition_(key, value, n_reducers = 2):\n",
    "    '''\n",
    "    - All kv pairs having same key sent to same partition                       -- DONE\n",
    "    - Different keys distributed equally among each partition (key%nreducers)   -- DONE\n",
    "    \n",
    "    '''\n",
    "    partitions = {}\n",
    "    for k, v in zip(key, value):\n",
    "        partition_id = k%n_reducers\n",
    "        if partition_id not in partitions.keys():\n",
    "            partitions[partition_id] = []\n",
    "        partitions[partition_id].append([k, v])\n",
    "        \n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = read_file(data_path)\n",
    "centroids = init_centroids(points, K)\n",
    "\n",
    "indices = np.random.randint(0, len(points), len(points)//3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "k, v = map_(data_path, indices, centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = partition_(k,v,2)\n",
    "p = partitions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, array([9.1, 3.1])],\n",
       " [2, array([9.8, 1.2])],\n",
       " [2, array([8.9, 0.2])],\n",
       " [2, array([9.5, 1.5])],\n",
       " [2, array([11.2, -1.2])]]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def shuffle_sort(part):\n",
    "    part = sorted(part, key = lambda x: x[0])\n",
    "    for "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mapreduce",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
